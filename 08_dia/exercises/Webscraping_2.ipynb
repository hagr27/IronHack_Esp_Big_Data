{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4349742-b63c-4464-a239-a3c59489ad70",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #00BFFF;\">Web Scraping REAL</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b0474f-1eaf-4f24-a9bc-067070a8d704",
   "metadata": {},
   "source": [
    "![let's go](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExOGllM2RkeHVmM3diZ2Mzdzhydm00MjFrbW8zMWgwNHEzbTl5eGEyMiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/fJKG1UTK7k64w/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cde7787",
   "metadata": {},
   "source": [
    "# Web Scraping de libros (Books to Scrape) – Versión para alumnos\n",
    "\n",
    "Objetivo de esta intro:\n",
    "1. Descargar el HTML de la **primera página** de libros.\n",
    "2. Extraer **títulos** y **precios** de cada libro.\n",
    "3. Crear un **DataFrame de pandas** con esos datos (y dejar el precio como número).\n",
    "4. (Opcional) Scraping de **múltiples páginas** con paginación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0b1c6",
   "metadata": {},
   "source": [
    "## Código de Conducta\n",
    "El fichero **robots.txt** es un archivo de texto que los sitios web colocan en su raíz (por ejemplo: https://example.com/robots.txt) y que sirve para indicar a los robots o crawlers (como Googlebot, Bingbot o scrapers) qué partes del sitio pueden o no pueden visitar.\n",
    "\n",
    "- `https://books.toscrape.com/robots.txt`\n",
    "- `https://openlibrary.org/robots.txt`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81588816",
   "metadata": {},
   "source": [
    "## 1) Preparación e importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "20b42c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librería estándar\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Librerías de terceros\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dcd73bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers para las peticiones HTTP\n",
    "HEADERS = {\n",
    "    \"Pragma\": \"no-cache\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"DNT\": \"1\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/51.0.2704.64 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept\": (\n",
    "        \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "        \"image/webp,image/apng,*/*;q=0.8,\"\n",
    "        \"application/signed-exchange;v=b3;q=0.9\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ea2a0bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Función para extraer detalle de cada libro por página individual -----------\n",
    "def get_book_details(book_url: str) -> dict:\n",
    "    \"\"\"\n",
    "    Obtiene información detallada de un libro desde su página individual:\n",
    "    - stock exacto\n",
    "    - categoría\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    book_url : str\n",
    "        URL de la página del libro.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    dict\n",
    "        {\n",
    "            \"stock\": int | None,\n",
    "            \"categoria\": str | None\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not book_url.startswith(\"http\"):\n",
    "        book_url = \"http://books.toscrape.com/catalogue/\" + book_url\n",
    "\n",
    "    try:\n",
    "        response = requests.get(book_url, headers=HEADERS)\n",
    "        response.encoding = \"utf-8\"\n",
    "        if response.status_code != 200:\n",
    "            return {\"stock\": None, \"categoria\": None}\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Stock\n",
    "        availability_text = soup.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
    "        match = re.search(r\"\\((\\d+) available\\)\", availability_text)\n",
    "        stock = int(match.group(1)) if match else None\n",
    "\n",
    "        # Categoría\n",
    "        breadcrumb = soup.find(\"ul\", class_=\"breadcrumb\")\n",
    "        categoria = None\n",
    "        if breadcrumb:\n",
    "            li_tags = breadcrumb.find_all(\"li\")\n",
    "            if len(li_tags) >= 3:\n",
    "                # categoría real del libro\n",
    "                categoria = li_tags[-2].get_text(strip=True)\n",
    "\n",
    "        return {\"stock\": stock, \"categoria\": categoria}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al obtener detalles de {book_url}: {e}\")\n",
    "        return {\"stock\": None, \"categoria\": None}\n",
    "\n",
    "    finally:\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Función para extraer información de una sola página -----------\n",
    "def scrape_page(url: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Extrae la información de todos los libros en una página de 'Books to Scrape'.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    url : str\n",
    "        URL de la página que se desea scrapear.\n",
    "\n",
    "    Devuelve\n",
    "    --------\n",
    "    list[dict]\n",
    "        Lista de diccionarios, donde cada diccionario contiene:\n",
    "        - \"titulo\": str, título del libro.\n",
    "        - \"precio\": float, precio en libras (£).\n",
    "        - \"disponible\": bool, True si está en stock.\n",
    "        - \"stock\": int | None\n",
    "        - \"categoria\": str | None\n",
    "        - \"estrellas\": int, número de estrellas (1 a 5).\n",
    "\n",
    "    Notas\n",
    "    -----\n",
    "    - Se fuerza la codificación a UTF-8 para evitar errores de caracteres.\n",
    "    - El stock y categoria exacto está disponible en la web individual.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    response.encoding = \"utf-8\"  # Forzar codificación UTF-8\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    books = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "    books_data = []\n",
    "    star_map = {\"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4, \"Five\": 5}\n",
    "\n",
    "    for book in books:\n",
    "        title = book.h3.a[\"title\"].strip()\n",
    "        title = title.encode(\"utf-8\").decode(\"utf-8\")\n",
    "\n",
    "        price_text = book.find(\"p\", class_=\"price_color\").get_text()\n",
    "        price = float(price_text.replace(\"£\", \"\"))\n",
    "\n",
    "        availability_text = book.find(\"p\", class_=\"instock availability\").get_text(strip=True)\n",
    "        disponible = \"In stock\" in availability_text\n",
    "\n",
    "        # Obtener stock y categoría desde la página individual\n",
    "        book_href = book.h3.a[\"href\"]\n",
    "        details = get_book_details(book_href)\n",
    "        stock = details[\"stock\"]\n",
    "        categoria = details[\"categoria\"]\n",
    "\n",
    "        # Extraer número de estrellas\n",
    "        star_tag = book.find(\"p\", class_=\"star-rating\")\n",
    "        stars_class = star_tag[\"class\"][1] if star_tag and len(star_tag[\"class\"]) > 1 else \"Zero\"\n",
    "        estrellas = star_map.get(stars_class, 0)\n",
    "\n",
    "        books_data.append(\n",
    "            {\n",
    "                \"titulo\": title,\n",
    "                \"precio\": price,\n",
    "                \"disponible\": disponible,\n",
    "                \"stock\": stock,  # Se puede incluir si se desea\n",
    "                \"categoria\": categoria,\n",
    "                \"estrellas\": estrellas,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return books_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2d1ac274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Función para obtener el número total de páginas -----------\n",
    "def get_total_pages(base_url: str) -> int:\n",
    "    \"\"\"\n",
    "    Obtiene el número total de páginas disponibles en el catálogo.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    base_url : str\n",
    "        URL de la primera página del catálogo \n",
    "        (ejemplo: \"http://books.toscrape.com/catalogue/page-1.html\").\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    int\n",
    "        Número total de páginas encontradas en el catálogo.\n",
    "        Si no se detecta el elemento, retorna 1 por defecto.\n",
    "    \"\"\"\n",
    "    response = requests.get(base_url, headers=HEADERS)\n",
    "    response.encoding = \"utf-8\"\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    page_info = soup.find(\"li\", class_=\"current\")\n",
    "\n",
    "    if page_info:\n",
    "        match = re.search(r\"Page \\d+ of (\\d+)\", page_info.text.strip())\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5dd2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Función para extraer información de todas las páginas dinámicamente -----------\n",
    "def scrape_all_books(base_url_pattern: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Recorre todas las páginas del catálogo dinámicamente y devuelve un DataFrame.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    base_url_pattern : str\n",
    "        Patrón de URL que debe incluir un marcador `{}` para el número de página.\n",
    "        Ejemplo: \"http://books.toscrape.com/catalogue/page-{}.html\".\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame con la información de todos los libros encontrados en el catálogo.\n",
    "        Incluye título, precio, disponibilidad, stock, categoria y número de estrellas.\n",
    "    \"\"\"\n",
    "    first_page_url = base_url_pattern.format(1)\n",
    "    total_pages = get_total_pages(first_page_url)\n",
    "    print(f\"Se detectaron {total_pages} páginas en total.\")\n",
    "\n",
    "    all_books = []\n",
    "    for page in range(1, total_pages + 1):\n",
    "        url = base_url_pattern.format(page)\n",
    "        print(f\"Scrapeando página {page}...\")\n",
    "        all_books.extend(scrape_page(url))\n",
    "\n",
    "    return pd.DataFrame(all_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b1bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------- Función para guardar resultados -----------\n",
    "def save_results(\n",
    "    df: pd.DataFrame,\n",
    "    folder_path: str = \".\",\n",
    "    file_name: str = \"books\",\n",
    "    save_csv: bool = True,\n",
    "    save_parquet: bool = False # Instalar dependencias\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Guarda el DataFrame en uno o varios formatos (CSV y/o Parquet) en la carpeta especificada.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame a guardar en archivo(s).\n",
    "    folder_path : str, opcional\n",
    "        Ruta de la carpeta donde se guardarán los archivos. \n",
    "        Por defecto, la carpeta actual `\".\"`.\n",
    "    file_name : str, opcional\n",
    "        Nombre base del archivo sin extensión. \n",
    "        Por defecto `\"books\"`.\n",
    "    save_csv : bool, opcional\n",
    "        Si es True, guarda el archivo en formato CSV. \n",
    "        Por defecto True.\n",
    "    save_parquet : bool, opcional\n",
    "        Si es True, guarda el archivo en formato Parquet. \n",
    "        Por defecto False.\n",
    "\n",
    "    Retorna\n",
    "    -------\n",
    "    None\n",
    "        Solo guarda los archivos en disco.\n",
    "    \"\"\"\n",
    "    # Crear carpeta si no existe\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # Guardar como CSV\n",
    "    if save_csv:\n",
    "        csv_file = os.path.join(folder_path, f\"{file_name}.csv\")\n",
    "        df.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "        print(f\"CSV guardado en: {csv_file}\")\n",
    "\n",
    "    # Guardar como Parquet\n",
    "    if save_parquet:\n",
    "        parquet_file = os.path.join(folder_path, f\"{file_name}.parquet\")\n",
    "        df.to_parquet(parquet_file, index=False)\n",
    "        print(f\"Parquet guardado en: {parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3fe516d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando todas las pruebas...\n",
      "\n",
      "Test: get_book_details()\n",
      "get_book_details() pasó correctamente.\n",
      "\n",
      "Test: scrape_page()\n",
      "scrape_page() pasó correctamente para los primeros 3 libros.\n",
      "\n",
      "Test: scrape_all_books() (máx. 5 páginas)\n",
      "scrape_all_books() pasó correctamente para las primeras 5 páginas.\n",
      "\n",
      "Todas las pruebas pasaron correctamente.......\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------- Funciones de prueba en Notebook -------------------\n",
    "\n",
    "def test_get_book_details(book_url: str):\n",
    "    \"\"\"Prueba la función get_book_details() sobre un libro individual.\"\"\"\n",
    "    print(\"Test: get_book_details()\")\n",
    "    details = get_book_details(book_url)\n",
    "    assert isinstance(details, dict), \"Debe retornar un diccionario\"\n",
    "    assert \"stock\" in details and \"categoria\" in details, \"Faltan claves en details\"\n",
    "    if details[\"stock\"] is not None:\n",
    "        assert isinstance(details[\"stock\"], int) and details[\"stock\"] >= 0, \"Stock debe ser int >= 0\"\n",
    "    if details[\"categoria\"] is not None:\n",
    "        assert isinstance(details[\"categoria\"], str) and len(details[\"categoria\"]) > 0, \"Categoria debe ser string no vacío\"\n",
    "    print(\"get_book_details() pasó correctamente.\\n\")\n",
    "\n",
    "\n",
    "def test_scrape_page(page_url: str, check_first_n: int = 3):\n",
    "    \"\"\"Prueba la función scrape_page() sobre una sola página.\"\"\"\n",
    "    print(\"Test: scrape_page()\")\n",
    "    books = scrape_page(page_url)\n",
    "    assert isinstance(books, list) and len(books) > 0, \"Debe retornar lista con al menos un libro\"\n",
    "    expected_keys = [\"titulo\", \"precio\", \"disponible\", \"stock\", \"estrellas\", \"categoria\"]\n",
    "    for i, book in enumerate(books[:check_first_n]):  # Solo revisar los primeros n libros\n",
    "        for key in expected_keys:\n",
    "            assert key in book, f\"Falta clave {key} en libro {i+1}\"\n",
    "        assert isinstance(book[\"titulo\"], str)\n",
    "        assert isinstance(book[\"precio\"], float) and book[\"precio\"] > 0\n",
    "        assert isinstance(book[\"disponible\"], bool)\n",
    "        if book[\"stock\"] is not None:\n",
    "            assert isinstance(book[\"stock\"], int) and book[\"stock\"] >= 0\n",
    "        assert isinstance(book[\"estrellas\"], int) and 0 <= book[\"estrellas\"] <= 5\n",
    "        if book[\"categoria\"] is not None:\n",
    "            assert isinstance(book[\"categoria\"], str)\n",
    "    print(f\"scrape_page() pasó correctamente para los primeros {check_first_n} libros.\\n\")\n",
    "\n",
    "\n",
    "def test_scrape_multiple_pages(base_url_pattern: str, max_pages: int = 5):\n",
    "    \"\"\"Prueba la función scrape_all_books() sobre un número limitado de páginas.\"\"\"\n",
    "    print(f\"Test: scrape_all_books() (máx. {max_pages} páginas)\")\n",
    "    all_books = []\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = base_url_pattern.format(page)\n",
    "        books = scrape_page(url)\n",
    "        all_books.extend(books)\n",
    "    import pandas as pd\n",
    "    df_books = pd.DataFrame(all_books)\n",
    "    expected_keys = [\"titulo\", \"precio\", \"disponible\", \"stock\", \"estrellas\", \"categoria\"]\n",
    "    assert not df_books.empty, \"El DataFrame no debe estar vacío\"\n",
    "    for col in expected_keys:\n",
    "        assert col in df_books.columns, f\"Falta columna {col} en DataFrame\"\n",
    "    print(f\"scrape_all_books() pasó correctamente para las primeras {max_pages} páginas.\\n\")\n",
    "    return df_books\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"Ejecuta todas las pruebas secuencialmente.\"\"\"\n",
    "    print(\"Iniciando todas las pruebas...\\n\")\n",
    "    # Test 1: libro individual\n",
    "    test_get_book_details(\"http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html\")\n",
    "    # Test 2: una página\n",
    "    test_scrape_page(\"http://books.toscrape.com/catalogue/page-1.html\")\n",
    "    # Test 3: varias páginas (limitadas a 5)\n",
    "    base_url_pattern = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "    df = test_scrape_multiple_pages(base_url_pattern, max_pages=5)\n",
    "    print(\"Todas las pruebas pasaron correctamente.......\\n\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ------------------- Ejecutar todas las pruebas -------------------\n",
    "df_books = run_all_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "48c076f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se detectaron 50 páginas en total.\n",
      "Scrapeando página 1...\n",
      "Scrapeando página 2...\n",
      "Scrapeando página 3...\n",
      "Scrapeando página 4...\n",
      "Scrapeando página 5...\n",
      "Scrapeando página 6...\n",
      "Scrapeando página 7...\n",
      "Scrapeando página 8...\n",
      "Scrapeando página 9...\n",
      "Scrapeando página 10...\n",
      "Scrapeando página 11...\n",
      "Scrapeando página 12...\n",
      "Scrapeando página 13...\n",
      "Scrapeando página 14...\n",
      "Scrapeando página 15...\n",
      "Scrapeando página 16...\n",
      "Scrapeando página 17...\n",
      "Scrapeando página 18...\n",
      "Scrapeando página 19...\n",
      "Scrapeando página 20...\n",
      "Scrapeando página 21...\n",
      "Scrapeando página 22...\n",
      "Scrapeando página 23...\n",
      "Scrapeando página 24...\n",
      "Scrapeando página 25...\n",
      "Scrapeando página 26...\n",
      "Scrapeando página 27...\n",
      "Scrapeando página 28...\n",
      "Scrapeando página 29...\n",
      "Scrapeando página 30...\n",
      "Scrapeando página 31...\n",
      "Scrapeando página 32...\n",
      "Scrapeando página 33...\n",
      "Scrapeando página 34...\n",
      "Scrapeando página 35...\n",
      "Scrapeando página 36...\n",
      "Scrapeando página 37...\n",
      "Scrapeando página 38...\n",
      "Scrapeando página 39...\n",
      "Scrapeando página 40...\n",
      "Scrapeando página 41...\n",
      "Scrapeando página 42...\n",
      "Scrapeando página 43...\n",
      "Scrapeando página 44...\n",
      "Scrapeando página 45...\n",
      "Scrapeando página 46...\n",
      "Scrapeando página 47...\n",
      "Scrapeando página 48...\n",
      "Scrapeando página 49...\n",
      "Scrapeando página 50...\n",
      "CSV guardado en: /home/neo/PROJECTS/IronHack_Esp_Big_Data/08_dia/exercises/books_catalog.csv\n"
     ]
    }
   ],
   "source": [
    "# URL y paths\n",
    "URL_PATTERN = \"http://books.toscrape.com/catalogue/page-{}.html\"\n",
    "\n",
    "SAVE_PATH = \"/home/neo/PROJECTS/IronHack_Esp_Big_Data/08_dia/exercises\"\n",
    "FILE_NAME = \"books_catalog\"\n",
    "\n",
    "#-----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Definir URL base dinámica\n",
    "base_url_pattern = URL_PATTERN\n",
    "\n",
    "# Scraping de todas las páginas\n",
    "df_books = scrape_all_books(base_url_pattern)\n",
    "\n",
    "# Guardar en carpeta específica con nombre de archivo personalizado\n",
    "save_results(df_books, folder_path=SAVE_PATH, file_name=FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89406d48",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b466c04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>precio</th>\n",
       "      <th>disponible</th>\n",
       "      <th>stock</th>\n",
       "      <th>categoria</th>\n",
       "      <th>estrellas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the Attic</td>\n",
       "      <td>51.77</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "      <td>Poetry</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>Historical Fiction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>Fiction</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>Mystery</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History of Humankind</td>\n",
       "      <td>54.23</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "      <td>History</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  titulo  precio  disponible  stock  \\\n",
       "0                   A Light in the Attic   51.77        True     22   \n",
       "1                     Tipping the Velvet   53.74        True     20   \n",
       "2                             Soumission   50.10        True     20   \n",
       "3                          Sharp Objects   47.82        True     20   \n",
       "4  Sapiens: A Brief History of Humankind   54.23        True     20   \n",
       "\n",
       "            categoria  estrellas  \n",
       "0              Poetry          3  \n",
       "1  Historical Fiction          1  \n",
       "2             Fiction          1  \n",
       "3             Mystery          4  \n",
       "4             History          5  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostrar los primeros registros\n",
    "df_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0bf414ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   titulo      1000 non-null   object \n",
      " 1   precio      1000 non-null   float64\n",
      " 2   disponible  1000 non-null   bool   \n",
      " 3   stock       1000 non-null   int64  \n",
      " 4   categoria   1000 non-null   object \n",
      " 5   estrellas   1000 non-null   int64  \n",
      "dtypes: bool(1), float64(1), int64(2), object(2)\n",
      "memory usage: 40.2+ KB\n",
      "\n",
      "Total de libros scrapeados: 1000\n"
     ]
    }
   ],
   "source": [
    "# Información general\n",
    "df_books.info()\n",
    "\n",
    "# Número total de libros scrapeados\n",
    "print(f\"\\nTotal de libros scrapeados: {len(df_books)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IronHack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
